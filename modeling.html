<!DOCTYPE html>
<!--[if lt IE 7]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html lang="en">
  <!--<![endif]-->

  <head>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <meta name="description" content="Haibin Zhao 赵海滨" />
    <meta name="author" content="Haibin Zhao" />

    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1"
    />

    <title>Haibin Zhao</title>

    <!-- STYLES -->
    <link
      href="https://fonts.googleapis.com/css2?family=Mulish:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Montserrat:200i,300,300i,400,400i,500,500i,600,700,700i,800&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" type="text/css" href="css/plugins.css" />
    <link rel="stylesheet" type="text/css" href="css/colors.css" />
    <link rel="stylesheet" type="text/css" href="css/darkMode.css" />
    <link rel="stylesheet" type="text/css" href="css/style.css" />
    <!--[if lt IE 9]>
      <script type="text/javascript" src="js/modernizr.custom.js"></script>
    <![endif]-->
    <!-- /STYLES -->
  </head>

  <body>
    <div class="arlo_tm_news">
      <div class="container">
        <div class="arlo_tm_main_title">
          <h3>Modeling of Analog Neuromorphic Circuit</h3>
        </div>
        <div class="list_inner">
          <div class="section-spacing"></div>
          <div class="section-spacing"></div>
          <h4>What is an analog neuromorphic circuit?</h4>
          <figure>
            <img
              src="img/myfigures/primitive.png"
              alt="Analog Neuromorphic Circuit"
            />
            <figcaption>
              Figure 1: Schematic of an analog neuromorphic circuit.
            </figcaption>
          </figure>
          <div class="mytext">
            Neuromorphic circuits are specialized electronic circuits aiming to
            emulate computational paradigms in artificial neural networks
            (ANNs), primarily multilayer perceptrons (MLPs). ANNs are not only
            capable of solving highly-complicated problems, but also composed of
            only streamlined elemental operations, namely, weighted-sums and
            nonlinear activations. Since these operations can be easily
            implemented through simple circuits, neuromorphic circuits have
            garnered significant interest within the EDA community.
          </div>

          <div class="section-spacing"></div>

          <h4>Difficulties in modeling the neuromorphic circuit?</h4>
          <div class="mytext">
            Although neuromorphic circuits are designed to emulate ANNs, due to
            physical constraints, not all ANNs can be perfectly mapped into a
            corresponding neuromorphic circuit.
          </div>
          <h5>1. Constraint of the "weights"</h5>
          <div class="mytext">
            Although neuromorphic circuits are designed to emulate ANNs, due to
            physical constraints, not all ANNs can be perfectly mapped into a
            corresponding neuromorphic circuit. Because the weights are embodied
            by the resistances through
            <p>$$ w_i = \frac{g_i}{\sum_j g_j},$$</p>
            where \( g_i \) is defined by \( 1/R_i \), named conductance.
            Therefore, it is evident that the weights, as well as the sum of the
            weights inside a neuron, are necessarily less than one. If the
            weights in an ANN doesn't meet this constraint, it can not be mapped
            into a physical circuit. Also, as the weights are represented by the
            ratio of the conductances, they are coupled with each other, meaning
            that it is difficult to change every single weight value. Therefore,
            instead of training ANN and converting it into a neuromorphic
            circuit, we rather directly take the conductances as learnable
            parameters.
          </div>
          <figure>
            <img
              src="img/myfigures/neg.png"
              alt="Analog Neuromorphic Circuit"
              style="width: 30%"
            />
            <figcaption>
              Figure 2: Schematic of an inverter-based negation circuit.
            </figcaption>
          </figure>
          <h5>2. Lack of negative resistance</h5>
          <div class="mytext">
            As the weights are emulated by the conductances, they can only be
            positive values, which impact the expressiveness of the computing
            system. Therefore, to express negative relations, we proposed an
            inverter-based negation circuit, which can invert the input signal
            to a negative one, so that the negative relationship can be
            converted to an alternative expression:
            <p>
              $$ (-w_i)\cdot x_i \to w_i\cdot (-x_i) \to w_i \cdot {\rm
              neg}(x_i). $$
            </p>
            Note that, as shown in Figure 2, the transfer characteristic of the
            negation circuit is not linear. Therefore, the \({\rm neg}(\cdot)\)
            function need to be considered into the training process, instead of
            simply prepend a negation circuit when negative weight is required.
            To facilitate the design of negation circuit, we use a surrogate
            conductance \( \theta \) as learnable parameter, which encodes the
            real conductance by its absolute value \( g=|\theta| \) and the
            existence of negation circuit through its sign \( \theta = {\rm
            sign} (\theta) \).
          </div>
          <h5>3. Limited Manufacturability</h5>
          <div class="mytext">
            Due to technical limitations, the manufacturable conductance values
            are limited in a certain range, denoted by \( [g_{\rm min},g_{\rm
            max}]\cup \{0\} \). Therefore, in the forward pass, we project the
            surrogate conductance through a piece-wise linear function to the
            manufacturable conductance space.
            <figure>
              <img
                src="img/myfigures/forward.png"
                alt="Analog Neuromorphic Circuit"
                style="width: 40%"
              />
              <figcaption>
                Figure 3: Projecting surrogate conductance to manufacturable
                range.
              </figcaption>
            </figure>
          </div>
          <div class="section-spacing"></div>
          <h4>Modeling of the Neuromorphic Circuit</h4>
          <div class="mytext">
            After obtaining the mathematical description of the neuromorphic
            circuit, we can model and optimize the learnable parameters (i.e.,
            conductances) through different approaches. The most straightforward
            way is to employ ML-based approach, as the neuromorphic circuit was
            targeting to mimic ANNs. Beside, evolutionary algorithms can also be
            employed to optimize the circuit.
          </div>
          <h5>1. Machine Learning Approach</h5>
          <div class="mytext">
            Through formulating the computing graph from the original learnable
            parameter \( \theta \) to the manufacturable conductance \(
            \tilde{\theta} \), and thus formulating the corresponding weights \(
            w \), the output of the forward pass of the computing graph
            simulates the output of the neuromorphic circuit. By designing the
            loss function, learnable parameters can be optimized through
            back-propagation. However, as this is a gradient-based training
            process, it is critical to keep every computing operation in the
            computing graph is differentiable. Therefore, additional measures to
            the non-differentiable operation, e.g., the projection of \( \theta
            \) to \( \tilde{\theta} \), must be taken. A promising method is to
            provide another function that can heuristically provide informative
            gradient information and thus enables training through gradient
            descent.
          </div>
          <figure>
            <img
              src="img/myfigures/backward.png"
              alt="Analog Neuromorphic Circuit"
              style="width: 40%"
            />
            <figcaption>
              Figure 4: Heuristic gradient that propates back from \(
              \tilde{\theta} \) to \( \theta \).
            </figcaption>
          </figure>

          <h5>2. Evolutionary Approach</h5>
          <div class="mytext">
            Compared to the gradient approach, evolutionary algorithms allows
            more flexible optimization problems. For example, the
            non-differentiable projection of surrogate condutance can be
            directly involved into the evolutionary algorithm withtout any
            additional measures. Also, the evolutionary algorithm can even
            optimize the circuit topology (neural architecture search) along
            with the training of conductances. However, the evolutionary
            algorithm is more computationally expensive, and requires many
            task-specific designs to achieve desired training objective.
          </div>
          <figure>
            <img
              src="img/myfigures/ea.png"
              alt="Analog Neuromorphic Circuit"
              style="width: 100%"
            />
            <figcaption>
              Figure 5: Evolutionary algorithm that allows training both
              parameters and neural architecture.
            </figcaption>
          </figure>
          <div class="mytext">
            Figure 5 illustrates an evolutionary algorithm. By encoding the
            neurons and connections into the genomes, both parameters and neural
            architecture can be optimized through speciation, crossover, and
            mutation.
          </div>

          <div class="section-spacing"></div>
          <h4>Related Materials</h4>
          <div class="section-spacing"></div>
          <div class="mytext">
            <ul>
              <li>
                (submitted) <b>H. Zhao</b> <i>et al</i>. Neural Evolutionary
                Architecture Search for Compact Printed Analog Neuromorphic
                Circuit. In Proceedings of Design Automation Conference (DAC),
                ACM, 2024.
              </li>
              <li>
                <a href="slides/phdintroduction.pdf" target="_blank"><b>[Introduction of my dissertation]</b></a>
              </li>
              <li>
                Supervised master thesis of <b>Z. Yang</b>, Learnable Nonlinear
                Circuit for Printed Neuromorphic Circuits. 2022.
                <a href="papers/2022_Master_Zhidong_Yang____Learnable_Non_linear_Circuity_for_printed_Neuromorphic_Circuits.pdf" download target="_blank" ><b>[PDF]</b></a>
              </li>
              <li>
                Supervised master thesis of <b>Y. Wang</b>, Neural Evolution for
                Augmenting Topologies in Printed Neuromorphic Circuits. 2023.
                <a href="papers/neuralevolution.pdf" download target="_blank" ><b>[PDF]</b></a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
